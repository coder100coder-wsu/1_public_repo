{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dependencies\n",
    "## Install pyspark, pandas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import dependencies/packages such as pyspark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import pyspark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Spark Session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('spark_project ').getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Note: getOrCreate() is important.\n",
    "Otherwise you have to manually reset kernel everytime, and manually run cells in proper sequence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SparkUI hyperlink available"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ALWAYS USE SPARK FUNCTIONS\n",
    "TO TAKE ADVANTAGE OF SPARK'S EXECUTION SPEED. STAY AWAY FROM USER-DEFINED FUNCTIONS IF POSSIBLE."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x286fcfc9ba0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>spark_project </code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Data, part of Data Wrangling, ETL process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# df_spark = spark.read.option('header','true').csv(\"file:///D:/2_R_repo/2_python repo/Spark project/auto-mpg.csv\", inferSchema=True)\n",
    "df_spark = spark.read.csv(\"file:///D:/2_general_repo/1_public_repo/Spark project/auto-mpg.csv\", inferSchema=True, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: without inferSchema, everything is type-string.\n",
    "df_spark.describe()\n",
    "DataFrame[summary: string, _c0: string, V1: string, V2: string, V3: string, V4: string, V5: string, V6: string, V7: string, V8: string, V9: string]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[_c0: int, V1: double, V2: int, V3: double, V4: string, V5: int, V6: double, V7: int, V8: int, V9: string]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get DataFrame structure, printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: integer (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: string (nullable = true)\n",
      " |-- V5: integer (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: integer (nullable = true)\n",
      " |-- V8: integer (nullable = true)\n",
      " |-- V9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(_c0=1, V1=18.0, V2=8, V3=307.0, V4='130', V5=3504, V6=12.0, V7=70, V8=1, V9='chevrolet chevelle malibu'),\n Row(_c0=2, V1=15.0, V2=8, V3=350.0, V4='165', V5=3693, V6=11.5, V7=70, V8=1, V9='buick skylark 320'),\n Row(_c0=3, V1=18.0, V2=8, V3=318.0, V4='150', V5=3436, V6=11.0, V7=70, V8=1, V9='plymouth satellite'),\n Row(_c0=4, V1=16.0, V2=8, V3=304.0, V4='150', V5=3433, V6=12.0, V7=70, V8=1, V9='amc rebel sst'),\n Row(_c0=5, V1=17.0, V2=8, V3=302.0, V4='140', V5=3449, V6=10.5, V7=70, V8=1, V9='ford torino')]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Show data, .show()\n",
    "similar to pandas .head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+-----+---+----+----+---+---+--------------------+\n",
      "|_c0|  V1| V2|   V3| V4|  V5|  V6| V7| V8|                  V9|\n",
      "+---+----+---+-----+---+----+----+---+---+--------------------+\n",
      "|  1|18.0|  8|307.0|130|3504|12.0| 70|  1|chevrolet chevell...|\n",
      "|  2|15.0|  8|350.0|165|3693|11.5| 70|  1|   buick skylark 320|\n",
      "|  3|18.0|  8|318.0|150|3436|11.0| 70|  1|  plymouth satellite|\n",
      "|  4|16.0|  8|304.0|150|3433|12.0| 70|  1|       amc rebel sst|\n",
      "|  5|17.0|  8|302.0|140|3449|10.5| 70|  1|         ford torino|\n",
      "+---+----+---+-----+---+----+----+---+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Describe data_frame, .describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[summary: string, _c0: string, V1: string, V2: string, V3: string, V4: string, V5: string, V6: string, V7: string, V8: string, V9: string]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get all column names, .columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "['_c0', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select certain columns, .select()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| V4|   V3|\n",
      "+---+-----+\n",
      "|130|307.0|\n",
      "|165|350.0|\n",
      "|150|318.0|\n",
      "|150|304.0|\n",
      "|140|302.0|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select(['V4','V3']).show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: .select() creates a \"view\", does NOT change the df in any way"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get data_types in DataFrame, .dtypes()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "[('_c0', 'int'),\n ('V1', 'double'),\n ('V2', 'int'),\n ('V3', 'double'),\n ('V4', 'string'),\n ('V5', 'int'),\n ('V6', 'double'),\n ('V7', 'int'),\n ('V8', 'int'),\n ('V9', 'string')]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get summary or description, describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|              _c0|                V1|                V2|                V3|                V4|               V5|                V6|                V7|                V8|                  V9|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|              225|               225|               225|               225|               225|              225|               225|               225|               225|                 225|\n",
      "|   mean|            113.0|19.964444444444446| 5.884444444444444|220.58444444444444|114.72197309417041|           3170.8|15.114666666666663|             73.24|1.4622222222222223|                null|\n",
      "| stddev|65.09608283145768| 6.043264977419209|1.7815968403618432|113.29117423140319|   42.680288629891|928.9297509192268|2.8520681718961174|2.0970216975510825|0.7255813214025385|                null|\n",
      "|    min|                1|               9.0|                 3|              68.0|               100|             1613|               8.0|                70|                 1|amc ambassador br...|\n",
      "|    max|              225|              36.0|                 8|             455.0|                 ?|             5140|              23.5|                77|                 3|           vw rabbit|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.describe().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: Spark does lazy evaluation/execution. Unless output is forced, no output is generated, only instruction set created ready for execution.\n",
    ".show() forces output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add Columns, .withColumn()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn('V10', df_spark['V2']+2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: spark allows column-operations (or \"vectorized\" operations). so loops are NOT required. if you're using loops in spark for column or row operations, re-evaluate!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+-----+---+----+----+---+---+--------------------+---+\n",
      "|_c0|  V1| V2|   V3| V4|  V5|  V6| V7| V8|                  V9|V10|\n",
      "+---+----+---+-----+---+----+----+---+---+--------------------+---+\n",
      "|  1|18.0|  8|307.0|130|3504|12.0| 70|  1|chevrolet chevell...| 10|\n",
      "|  2|15.0|  8|350.0|165|3693|11.5| 70|  1|   buick skylark 320| 10|\n",
      "|  3|18.0|  8|318.0|150|3436|11.0| 70|  1|  plymouth satellite| 10|\n",
      "|  4|16.0|  8|304.0|150|3433|12.0| 70|  1|       amc rebel sst| 10|\n",
      "|  5|17.0|  8|302.0|140|3449|10.5| 70|  1|         ford torino| 10|\n",
      "+---+----+---+-----+---+----+----+---+---+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drop columns, .drop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop('V10')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+-----+---+----+----+---+---+--------------------+\n",
      "|_c0|  V1| V2|   V3| V4|  V5|  V6| V7| V8|                  V9|\n",
      "+---+----+---+-----+---+----+----+---+---+--------------------+\n",
      "|  1|18.0|  8|307.0|130|3504|12.0| 70|  1|chevrolet chevell...|\n",
      "|  2|15.0|  8|350.0|165|3693|11.5| 70|  1|   buick skylark 320|\n",
      "|  3|18.0|  8|318.0|150|3436|11.0| 70|  1|  plymouth satellite|\n",
      "|  4|16.0|  8|304.0|150|3433|12.0| 70|  1|       amc rebel sst|\n",
      "|  5|17.0|  8|302.0|140|3449|10.5| 70|  1|         ford torino|\n",
      "+---+----+---+-----+---+----+----+---+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rename column, .withColumnRenamed()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+-----+---+----+----+---+---+--------------------+\n",
      "|_c0| mpg| V2|   V3| V4|  V5|  V6| V7| V8|                  V9|\n",
      "+---+----+---+-----+---+----+----+---+---+--------------------+\n",
      "|  1|18.0|  8|307.0|130|3504|12.0| 70|  1|chevrolet chevell...|\n",
      "|  2|15.0|  8|350.0|165|3693|11.5| 70|  1|   buick skylark 320|\n",
      "|  3|18.0|  8|318.0|150|3436|11.0| 70|  1|  plymouth satellite|\n",
      "|  4|16.0|  8|304.0|150|3433|12.0| 70|  1|       amc rebel sst|\n",
      "|  5|17.0|  8|302.0|140|3449|10.5| 70|  1|         ford torino|\n",
      "+---+----+---+-----+---+----+----+---+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.withColumnRenamed('V1','mpg').show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rename ALL columns, toDF(*new_col_names)\n",
    "Can be used for multiple (as in less than all) columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "new_col_names = [\"sr_no\", \"mpg\", \"cyl\", \"dspl\", \"hp\", \"wt\", \"accl\", \"yr\", \"origin\", \"name\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: when defining variable containing NEW column names you can use () or []."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+-----+---+----+----+---+------+--------------------+\n",
      "|sr_no| mpg|cyl| dspl| hp|  wt|accl| yr|origin|                name|\n",
      "+-----+----+---+-----+---+----+----+---+------+--------------------+\n",
      "|    1|18.0|  8|307.0|130|3504|12.0| 70|     1|chevrolet chevell...|\n",
      "|    2|15.0|  8|350.0|165|3693|11.5| 70|     1|   buick skylark 320|\n",
      "|    3|18.0|  8|318.0|150|3436|11.0| 70|     1|  plymouth satellite|\n",
      "|    4|16.0|  8|304.0|150|3433|12.0| 70|     1|       amc rebel sst|\n",
      "|    5|17.0|  8|302.0|140|3449|10.5| 70|     1|         ford torino|\n",
      "+-----+----+---+-----+---+----+----+---+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.toDF(*new_col_names).show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Without assignment OR without capturing return value of function, the result is only view, not modification to df.\n",
    "Also Spark uses RDD, immutable datastructures, so everytime a brand new datastructure is created"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "df_spark = df_spark.toDF(*new_col_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+-----+---+----+----+---+------+--------------------+\n",
      "|sr_no| mpg|cyl| dspl| hp|  wt|accl| yr|origin|                name|\n",
      "+-----+----+---+-----+---+----+----+---+------+--------------------+\n",
      "|    1|18.0|  8|307.0|130|3504|12.0| 70|     1|chevrolet chevell...|\n",
      "|    2|15.0|  8|350.0|165|3693|11.5| 70|     1|   buick skylark 320|\n",
      "|    3|18.0|  8|318.0|150|3436|11.0| 70|     1|  plymouth satellite|\n",
      "|    4|16.0|  8|304.0|150|3433|12.0| 70|     1|       amc rebel sst|\n",
      "|    5|17.0|  8|302.0|140|3449|10.5| 70|     1|         ford torino|\n",
      "+-----+----+---+-----+---+----+----+---+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check for NaN values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+----+---+---+----+---+------+----+\n",
      "|sr_no|mpg|cyl|dspl| hp| wt|accl| yr|origin|name|\n",
      "+-----+---+---+----+---+---+----+---+------+----+\n",
      "|    0|  0|  0|   0|  0|  0|   0|  0|     0|   0|\n",
      "+-----+---+---+----+---+---+----+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select( [ count( when( isnan(c), c)).alias(c) for c in df_spark.columns]).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Difference between Null and Nan values\n",
    "Null is nothing\n",
    "NaN is Not a Number"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+----+---+---+----+---+------+----+\n",
      "|sr_no|mpg|cyl|dspl| hp| wt|accl| yr|origin|name|\n",
      "+-----+---+---+----+---+---+----+---+------+----+\n",
      "|    0|  0|  0|   0|  0|  0|   0|  0|     0|   0|\n",
      "+-----+---+---+----+---+---+----+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select( [ count( when( col(c).isNull(), c)).alias(c) for c in df_spark.columns]).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[sr_no: int, mpg: double, cyl: int, dspl: double, hp: string, wt: int, accl: double, yr: int, origin: int, name: string]"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.na.drop(how=\"all\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter operations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+-----+---+----+----+---+------+--------------------+\n",
      "|sr_no| mpg|cyl| dspl| hp|  wt|accl| yr|origin|                name|\n",
      "+-----+----+---+-----+---+----+----+---+------+--------------------+\n",
      "|   15|24.0|  4|113.0| 95|2372|15.0| 70|     3|toyota corona mar...|\n",
      "|   16|22.0|  6|198.0| 95|2833|15.5| 70|     1|     plymouth duster|\n",
      "|   18|21.0|  6|200.0| 85|2587|16.0| 70|     1|       ford maverick|\n",
      "|   19|27.0|  4| 97.0| 88|2130|14.5| 70|     3|        datsun pl510|\n",
      "|   20|26.0|  4| 97.0| 46|1835|20.5| 70|     2|volkswagen 1131 d...|\n",
      "+-----+----+---+-----+---+----+----+---+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter(\"mpg>20\").show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "95"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.filter(\"mpg>20\").count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter similar to Pandas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "95"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.filter(df_spark['mpg']>20).count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "[('sr_no', 'int'),\n ('mpg', 'double'),\n ('cyl', 'int'),\n ('dspl', 'double'),\n ('hp', 'string'),\n ('wt', 'int'),\n ('accl', 'double'),\n ('yr', 'int'),\n ('origin', 'int'),\n ('name', 'string')]"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "11"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.filter( (df_spark['mpg']>20.0) & (df_spark['cyl']==6) ).count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+-----+---+----+----+---+------+----------------+\n",
      "|sr_no| mpg|cyl| dspl| hp|  wt|accl| yr|origin|            name|\n",
      "+-----+----+---+-----+---+----+----+---+------+----------------+\n",
      "|   16|22.0|  6|198.0| 95|2833|15.5| 70|     1| plymouth duster|\n",
      "|   18|21.0|  6|200.0| 85|2587|16.0| 70|     1|   ford maverick|\n",
      "|   25|21.0|  6|199.0| 90|2648|15.0| 70|     1|     amc gremlin|\n",
      "|  102|23.0|  6|198.0| 95|2904|16.0| 73|     1| plymouth duster|\n",
      "|  114|21.0|  6|155.0|107|2472|14.0| 73|     1|mercury capri v6|\n",
      "+-----+----+---+-----+---+----+----+---+------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter( (df_spark['mpg']>20.0) & (df_spark['cyl']==6) ).show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| mpg|count|\n",
      "+----+-----+\n",
      "|15.5|    2|\n",
      "|31.5|    1|\n",
      "|29.0|    6|\n",
      "+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupBy('mpg').count().show(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+--------+-----------------+------------------+------------------+-----------------+------------------+\n",
      "|cyl|        avg(sr_no)|          avg(mpg)|avg(cyl)|        avg(dspl)|           avg(wt)|         avg(accl)|          avg(yr)|       avg(origin)|\n",
      "+---+------------------+------------------+--------+-----------------+------------------+------------------+-----------------+------------------+\n",
      "|  6|124.85714285714286|18.653061224489797|     6.0|225.9795918367347|3205.5102040816328|16.448979591836736| 73.6734693877551|1.1020408163265305|\n",
      "|  3|              92.0|              18.5|     3.0|             70.0|            2227.0|              13.5|             72.5|               3.0|\n",
      "|  4|123.17391304347827| 25.97826086956522|     4.0|106.0054347826087|2270.3152173913045|16.686956521739134|73.46739130434783| 2.032608695652174|\n",
      "|  8| 95.01219512195122|14.036585365853659|     8.0|349.5853658536585| 4183.378048780488| 12.59268292682927| 72.7439024390244|               1.0|\n",
      "+---+------------------+------------------+--------+-----------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupBy('cyl').mean().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression in PySpark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre_process data\n",
    "to make suitable for analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avengers, assemble!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: outputCol in VectorAssembler is the name of column,\n",
    "containing \"vector\" of input or independent features or predictors.\n",
    "It is added as a new column to original dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "features_assemble = VectorAssembler( inputCols=['cyl', 'dspl','wt'], outputCol='input_features')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vector Assemble features\n",
    "Transform DataFrame with vector-assembled features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "df_features_assemble = features_assemble.transform(df_spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "pyspark.sql.dataframe.DataFrame"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_features_assemble)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: VectorAssembler_object.transform(DataFrame) returns a new DataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+-----+---+----+----+---+------+--------------------+------------------+\n",
      "|sr_no| mpg|cyl| dspl| hp|  wt|accl| yr|origin|                name|    input_features|\n",
      "+-----+----+---+-----+---+----+----+---+------+--------------------+------------------+\n",
      "|    1|18.0|  8|307.0|130|3504|12.0| 70|     1|chevrolet chevell...|[8.0,307.0,3504.0]|\n",
      "|    2|15.0|  8|350.0|165|3693|11.5| 70|     1|   buick skylark 320|[8.0,350.0,3693.0]|\n",
      "|    3|18.0|  8|318.0|150|3436|11.0| 70|     1|  plymouth satellite|[8.0,318.0,3436.0]|\n",
      "+-----+----+---+-----+---+----+----+---+------+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features_assemble.show(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice the \"input_features\" column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre_process complete"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ready for Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "df_ready_for_analysis = df_features_assemble.select('mpg','input_features')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "| mpg|    input_features|\n",
      "+----+------------------+\n",
      "|18.0|[8.0,307.0,3504.0]|\n",
      "|15.0|[8.0,350.0,3693.0]|\n",
      "|18.0|[8.0,318.0,3436.0]|\n",
      "+----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ready_for_analysis.show(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Refer above code to know that \"input_features\" is vector of inputCols=['cyl', 'dspl','wt']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import LinearRegression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train-Test Split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "train_data, test_data = df_ready_for_analysis.randomSplit([0.75,0.25], seed=2022)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "167"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "58"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "225"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ready_for_analysis.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "| mpg|    input_features|\n",
      "+----+------------------+\n",
      "|18.0|[8.0,307.0,3504.0]|\n",
      "|15.0|[8.0,350.0,3693.0]|\n",
      "|18.0|[8.0,318.0,3436.0]|\n",
      "+----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ready_for_analysis.show(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Instantiate object for LinearRegression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "lm_object = LinearRegression(featuresCol='input_features', labelCol='mpg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.regression.LinearRegression'>\n"
     ]
    }
   ],
   "source": [
    "print( type(lm_object))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fit the model to training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "model_lm_fit_train_data = lm_object.fit(dataset=train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.regression.LinearRegressionModel'>\n"
     ]
    }
   ],
   "source": [
    "print( type(model_lm_fit_train_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionModel: uid=LinearRegression_c66b96d14d8b, numFeatures=3\n"
     ]
    }
   ],
   "source": [
    "print( model_lm_fit_train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               mpg|\n",
      "+-------+------------------+\n",
      "|  count|               225|\n",
      "|   mean|19.964444444444446|\n",
      "| stddev| 6.043264977419209|\n",
      "|    min|               9.0|\n",
      "|    max|              36.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ready_for_analysis.describe().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get RMSE\n",
    "Root Mean Squared Error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=  2.538\n"
     ]
    }
   ],
   "source": [
    "rmse_lm = round(model_lm_fit_train_data.summary.rootMeanSquaredError,3)\n",
    "print(\"RMSE= \", rmse_lm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get MAE\n",
    "Mean Absolute Error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE=  1.996\n"
     ]
    }
   ],
   "source": [
    "mae_lm = round(model_lm_fit_train_data.summary.meanAbsoluteError,3)\n",
    "print(\"MAE= \", mae_lm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get R-squared"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2=  0.811\n"
     ]
    }
   ],
   "source": [
    "r2_lm = round(model_lm_fit_train_data.summary.r2, 3)\n",
    "print(\"R2= \", r2_lm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get coefficients for fitted model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_coeff=  [-0.352, -0.009, -0.004]\n"
     ]
    }
   ],
   "source": [
    "model_coeff = model_lm_fit_train_data.coefficients\n",
    "print(\"model_coeff= \", [round(i,3) for i in model_coeff])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: \"input_features\" is vector of inputCols=['cyl', 'dspl','wt'].\n",
    "So coefficients above are 3-qty.total, 1 for each of the 3 predictors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get Intercept for fitted model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_intercept=  37.13\n"
     ]
    }
   ],
   "source": [
    "model_intercept = round(model_lm_fit_train_data.intercept,3)\n",
    "print(\"model_intercept= \", model_intercept)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make Predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "yhat_model_predicts = model_lm_fit_train_data.evaluate(dataset= test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.ml.regression.LinearRegressionSummary at 0x286fe25f220>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_model_predicts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+\n",
      "| mpg|    input_features|        prediction|\n",
      "+----+------------------+------------------+\n",
      "|10.0|[8.0,307.0,4376.0]|13.300805901781231|\n",
      "|10.0|[8.0,360.0,4615.0]| 11.83110154839293|\n",
      "|12.0|[8.0,350.0,4456.0]|12.584412779198807|\n",
      "+----+------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "yhat_model_predicts.predictions.show(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegressionSummary' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [80]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43myhat_model_predicts\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'LinearRegressionSummary' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "yhat_model_predicts.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}